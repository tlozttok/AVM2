# 语义自举：一种在实在界中生长智能的尝试

## 一、范式为何必要：当"程序"不再等于"结构+算法"

过去几十年，我们习惯将程序理解为**数据结构加上算法**。这种范式强大、清晰、可验证——但它预设了一个前提：**世界可以被分解为类型、字段、函数和状态机**。然而，当我们试图构建能自主演化、能处理开放世界任务的智能系统时，这一前提开始崩塌。

类型太多，规则更多。任何试图穷尽分类的努力，都会在真实世界的复杂性面前失效。更关键的是，**智能似乎并不依赖于显式规则**——人脑不会先定义"手臂"类型再调用 `arm.move()`，而是通过试错、反馈、关联，逐渐学会"某种内部状态能导致外部变化"。

与此同时，大语言模型展现出一种新可能：它能处理纯文本，无需预设结构，却能生成看似合理的代码、推理甚至规划。但它也有根本局限：**它只是续写，不真正行动；它可能流畅地幻觉，却无法被世界纠正**。

于是，一个更根本的问题浮现：**能否构建一个系统，让语义本身在与真实世界的互动中自我组织、自我修正、自我扩展？**  
不是靠我们写规则，而是靠世界说"不"——拉康叫它实在界的创伤

这便是"语义自举"范式的起点——它不试图解释智能，而是**构造一个场域，让智能有机会从符号与实在的耦合中涌现**。

在这个场域中，所有知识都是纯字符串，没有JSON，没有类，没有类型标签。一段字符串应该怎样被计算，完全被它所在的上下文以及LLM内置的大量语义知识决定。

我们不需要不需要"理解"系统在做什么，只需要让其：  
1. 能从基因库中拉取字符串，组合成上下文；  
2. 能将输出发送出去；  
3. 能接收来自实在界的反馈（如代码是否运行成功）；  
4. 能根据反馈，让某些符号链（字符串）更可能被再次使用，某些则被淘汰。

我们不用担心这个系统是黑箱，因为一切处理都依赖于人类给LLM训练出的象征界，AI的符号秩序和人类的是对齐的，可以互相影响。

这听起来简单，但其后果深远：**智能不再是设计的结果，而是淘汰的残余**。

---

## 二、框架如何可能：三界、三层与微Agent的运动

要让上述场域运转，我们需要一个最小但完整的耦合结构。这里引入拉康三界，作为**功能隐喻**，帮助我们划分系统中的不同作用域。

- **象征界**是假想的基因库——所有经验、知识、历史都以纯语义字符串形式存在，它不一定被存储。  
- **想象界**是上下文——微Agent激活时临时构建的字符串组合。它也包括那些可能被"甲基化"的基因，在这个系统中，基因库里大部分基因都是在想象界和象征界之间的。  
- **实在界**是物理世界的反馈：代码能否编译、测试是否通过、是否超时。它不讲道理，只说"是"或"否"。

微Agent的行为完全在想象界中展开。但它**没有办法直接触碰实在界**。所以我们构建一个**运动模块**（人工编写）持续监听想象界的输出，一旦检测到"可执行片段"（如包含 `def` 或 `print` 的字符串），就将其投射到实在界执行，并将结果（成功/错误/输出）编码为新字符串，送回象征界或相关Agent。感官也是如此，Agent不会主动感知，而是不断处理实在界程序给他的输入

这模仿了生物体的感官-运动闭环：神经放电 → 肌肉收缩 → 感觉反馈 → 神经调整。系统不预设"意图"，只记录"什么组合导致了什么结果"。

微Agent本身是"RNA式"的原初实体：它既能处理语义（作为上下文的一部分），又能触发操作（通过输出可执行字符串）。它的"主体性"极弱——不决策，不规划，只是在被激活时，根据当前上下文生成新字符串。

激活有三种方式：  
- **主体激活**：主动拉取依赖信息，适用于主导任务流；  
- **被动激活**：监听消息，满足条件即响应，适用于事件驱动；  
- **触发激活**：由时间、语境或内部信号联想触发，模拟"突然想起"，支持跨任务迁移。

激活是一种实在界的东西。所以有分类。这是工程妥协，后期会调整。

每次激活的上下文由三部分构成：  
- **背景知识**：相对静态，作为系统提示词附加，仅当被其他Agent更新时才变化；  
- **原始基因**：从基因库拉取的语义片段；  
- **输入信息**：来自用户或其他Agent的消息，每次不同。

默认情况下，**Agent不保留自己的输出**。生成的回应通过消息发送给其他Agent，由接收方纳入其下一次激活的上下文。这确保了上下文精简，避免LLM长上下文性能衰减。仅对轻量任务（如多轮对话片段），允许临时保留对话记录作为"情景即时记忆"。

---

## 三、演化如何发生：淘汰、奖励与自我叙事

系统如何从随机字符串走向有效行为？答案不是优化，而是**淘汰**。

>优化仍然存在，优化是被agent在想象界内部进行的，是符号链的自我优化
>但是自我优化肯定是会退化的，所以会通过进化算法不断和实在界对齐
>理想上，智能系统看到世界和预测不符，会改变预测，但是，由于AI不可靠性，系统可能会"死犟"不愿意更改预测。此时实在界的创伤就会积累，导致系统被清洗、淘汰

当某次行为（如生成一段代码）通过实在界检验，系统会生成一条**奖励事件**，例如：  
> `"行为[生成数据库连接]成功，参与链：Agent_A → Agent_B → Agent_C"`

该事件被送入一个信用分配机制（初期可为简单衰减规则：C得1.0，B得0.8，A得0.64），生成个体记录，写入对应Agent的背景知识和元数据。背景知识是语义的，可以让系统中其他Agent优化这个Agent，元数据是形式的，让实在界淘汰这个Agent

反之，长期无奖励的Agent，其无效激活次数累积到阈值后，被标记为**待删除**。但删除不是静默操作，而是生成一条**死亡事件**：  
> `"Agent[ID=xyz]因长期无效被移除，其最后上下文为：[...]，最后行为：[...]"`

该事件被广播至相关Agent，可能触发它们的背景知识更新（如"不再向xyz发送消息"）或反思（如"A的输出是否导致了无效链？"）。**系统由此获得对自身历史的感知——不是通过内省，而是通过事件的内化**。（提醒易错点：这个系统的Agent都是微Agent，他们不会说"我"来代表自己，但是通过某些连接度很高的节点，自我同一性可以被近似维持）

当想象界陷入幻觉（如Agent坚信错误代码正确），进化算法作为最后保底启动：清洗Agent、打乱基因库、引入变异重组。变异可以由专门的**变异Agent**执行，其提示词如：  
> `"从以下输入中提取语义片段，随机插入或替换基因库中的条目"`

输入可来自外部（如一篇关于上下文工程的博客）或内部（如高频成功片段）。Transformer足以将此类指令转化为有效字符串操作，无需显式编程。

---

## 四、工程实现：从理论到代码

### 系统架构设计
```python
# 核心设计原则
1. 三界分离：实在界、想象界、象征界
2. 微Agent架构：RNA式原初实体
3. 语义-实在转换：系统Agent作为交界点
4. 文件同步机制：语义体系持久化
```

### 代码风格规范
```python
# Python最佳实践
- 类型注解：所有方法参数和返回值
- 抽象基类：ABC和abstractmethod定义接口
- 异步编程：统一使用async/await
- 异常处理：明确的错误信息
- 模块化设计：driver/核心，system_interface_agents/实现
```

### 项目结构
```
AVM2/
├── driver/                    # 核心组件
│   ├── driver.py             # Agent基类、消息总线
│   ├── system_agents.py      # 系统Agent抽象类
│   └── async_system.py       # 异步系统
├── system_interface_agents/  # 系统Agent具体实现
├── Agents/                   # Agent配置
│   ├── *.yaml               # 普通Agent
│   └── SystemAgents/        # 系统Agent配置
└── main.py                  # 系统入口
```

### 关键工程洞察
1. **伪代码理解**：伪代码是示意，不能运行，需要实现为可执行代码
2. **抽象类设计**：InputAgent/OutputAgent是抽象基类，不能直接实例化
3. **动态类型创建**：使用eval(class_name)根据YAML配置动态创建Agent
4. **简单性原则**：避免过度设计，直接实现需求
5. **文件交互**：所有Agent类必须实现sync_to_file和sync_from_file方法

### 系统启动流程
1. **扫描配置**：递归扫描Agents文件夹下的所有YAML文件
2. **动态创建**：根据class_name使用eval()创建Agent实例
3. **状态同步**：调用sync_from_file加载Agent状态
4. **系统注册**：将Agent注册到AgentSystem
5. **启动运行**：启动消息总线，运行系统

---

## 五、未解之结：范式的边界与张力

尽管框架已具雏形，若干根本张力仍未消解：

1. **LLM的双重角色**：  
   系统依赖LLM修复碎片化语义、生成可执行组合，但又期望LLM的先验在后期成为"噪声"。若LLM无法修复变异后的无效字符串，系统是否会崩溃？**自举是否隐含对LLM先验的永久依赖？**

>llm的能力会不断发展，该系统的上限也会不断增强，此外，如果引入更复杂的连续token、微调模型，系统应该是AGI。

2. **信用分配的起源**：  
   回溯奖励机制需人工设定衰减规则。但该规则本身是否应被演化？若不能，是否意味着存在一个"不可演化的优化内核"？**自举的完整性是否因此受限？**

>当系统稳定下来后，进化机制可以不这么细节，信用分配可以很稀疏，因为优化机制会接手。

3. **长期规划的涌现条件**：  
   无状态微Agent如何协调罕见的多步任务？若某策略仅在百万次中成功一次，系统能否记住？**系统是否天然偏向高频、短程行为，难以发展复杂规划？**

>有状态Agent也做不到这个。复杂规划反而是微Agent的长处，因为它显式地维护高层规划和高低映射，而在单上下文Agent中一切都混在一起，全靠注意力机制隔离。探索罕见策略的话，可以靠系统分裂出一个语境噪声大的子系统，由于上下文噪声大，子系统的策略熵会高

4. **文化锚点的来源**：  
   我们期望"语义最小片段"（如"走为上策"）在清洗中自然留存。但如果初始基因库全是随机字符，系统能否从零生成高适应性片段？**"文化锚点"是否必须人工播种？**

> 理论上进化可以解决问题，但是工程上最好让人来写

---

## 六、工程实践总结

### 学到的教训
1. **不要硬编码**：使用配置驱动而非硬编码逻辑
2. **理解抽象类**：抽象类不能直接实例化，需要具体实现
3. **保持代码简洁**：避免不必要的复杂性
4. **测试驱动**：先确保代码能运行，再考虑优化
5. **动态类型使用**：Python的eval()可以简化动态对象创建

### 设计原则
1. **语义自举**：智能从符号与实在的耦合中涌现
2. **三界分离**：明确划分功能边界
3. **微Agent架构**：弱主体性，强组合性
4. **演化机制**：淘汰优于优化

### 未来方向
1. **进化算法集成**：更复杂的信用分配和淘汰机制
2. **语义修复**：LLM在演化过程中的角色优化
3. **多模态支持**：扩展支持图像、音频等输入输出
4. **分布式架构**：支持多节点Agent系统

---

**核心洞察**：智能不再是设计的结果，而是淘汰的残余。语义自举通过构造一个场域，让智能有机会从符号与实在的耦合中涌现。