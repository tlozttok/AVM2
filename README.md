这个项目的核心思想是LLM将字符串映射到计算。因此，重要的不是提高对话能力，而是利用字符串上下文进行语义编程

语义编程就是指提示词对模型表现的引导

但是对于每个问题，人工写提示词是不可行的

我们想到：使用模型写提示词。

但是大部分情况下，这只会变成：LLM内部知识被输入引导，输出提示词，LLM内部知识会覆盖外部输入，导致系统幻觉

所以我们要尽量减少LLM内部知识的输出。通过让LLM对输入进行简单地存储和压缩，LLM在大部分情况下能够展现出对外部输入的语义计算而不是输出内部知识——但是对于可以预测的某些hacking情况不行。（我认为这和没有免费午餐定理有关）

随后，让LLM能够操作自己在网络中的连接，这促进LLM的自举性角色生成，某种意义上是在单个context下进行具身输入输出

最后，让网络能够接入输入输出，并将网络视作一个整体。我认为通过一些设计，一个网络可以抵消LLM的内部知识，做到完全依靠经验的学习，以及根据经验而不是本能进行的行为。

然而，网络自身的优化是个问题。我认为LLM可以做为优化器——不是让LLM优化自己，而是LLM对输入字符进行运算，得到优化的提示字符，这完全没问题。只是我暂且不知道如何构造这样的东西。

总的来说，我们应该转变思维模式：从`提示->模型->生成内容`改为`非结构化信息->模型->非结构化信息`。前者的例子：生成代码、数学证明、文章、聊天。后者的例子：翻译、改写、信息提取、总结摘要。

Transformer模型们在前者确实很强，但在后者他们几乎不会出错。LLM们看过许多代码，把它们变成激活模式，推理时候把激活模式翻译成代码。输入信息会很快被模型内部知识覆盖。如果模型想要从一句话里生成一个软件，那么它必然要寻找大量内部知识。

模型根本不会推理。模型能推理。人类经常在有某些知识后就不理解没有那些知识的人了。至少我没有在模型的思考过程中发现我思考项目的那些方法。但是人们貌似以为LLM能自己学到这些？不可能！即使transfomer有AGI的潜力，想象你只是天天预测代码如何编写而从未运行和调试过代码，只能看别人的调试记录、输出记录，你怎么可能能有程序员的思维方法？想象你写了多个版本的代码，其中只有一个正确，然后你的记忆全部清空，历史记录变成了“你要这样写，不能这样写”的梯度信号（简单而言就是奖励信号没有语义信息并且没有给模型提供语义化奖励的机制，这个方向已经有研究了）。

在计算机眼里，知识应该是一堆数字，但是LLM提供了语言做为中介，语言可以用来思考，而思考不是生成，是运算。

有些东西可以用很简单的语言描述，比如算法可以用编程语言描述，但是现实不可以。“猫”的本质是一个非常复杂的物体分类器。